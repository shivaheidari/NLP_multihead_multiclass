{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29316c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiva/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce82532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "\n",
    "    \"model_path\" : \"../models/biobert\",\n",
    "    \"max_lenght\" : 128,\n",
    "    \"encoder_lr\" : 2e-5,\n",
    "    \"head_lr\"    : 5e-5\n",
    "     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiva.heydari/Desktop/cls_mdu/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/shiva.heydari/Desktop/cls_mdu/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModel.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac20e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dir = \"../models/biobert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83bad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/biobert/tokenizer_config.json',\n",
       " '../models/biobert/special_tokens_map.json',\n",
       " '../models/biobert/vocab.txt',\n",
       " '../models/biobert/added_tokens.json',\n",
       " '../models/biobert/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30d25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/biobert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e47ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1357.52it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: ../models/biobert\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.decoder.bias               | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "encoder = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c969e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\n",
    "  \"text\": \"short merge text ...\",\n",
    "  \"modality\": \"MR\",\n",
    "  \"vendor\": \"GE\",\n",
    "  \"series_type\": \"FLAIR\",\n",
    "  \"plane\": \"AX\",\n",
    "  \"acquisition\": \"2D\",\n",
    "  \"body\": \"brain\",\n",
    "  \"contrast\": \"with\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b52c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample = {\n",
    "  \"text\": \"short merge text ...\",\n",
    "  \"label_modality\": 0,\n",
    "  \"label_vendor\": 1,\n",
    "  \"label_series_type\": 2,\n",
    "  \"label_plane\": 0,\n",
    "  \"label_acquisition\": 1,\n",
    "  \"label_body\": 3,\n",
    "  \"label_contrast\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3fff2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = {\n",
    "    \"modality\": 3,\n",
    "    \"vendor\":2,\n",
    "    \"series_type\": 3,\n",
    "    \"plane\": 2,\n",
    "    \"acquisition\": 3,\n",
    "    \"body\": 4,\n",
    "    \"contrast\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fd6e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   text  label_modality  label_vendor  label_series_type  \\\n",
      "0  short merge text ...               0             1                  2   \n",
      "\n",
      "   label_plane  label_acquisition  label_body  label_contrast  \n",
      "0            0                  1           3               0  \n"
     ]
    }
   ],
   "source": [
    "df_sample = pd.DataFrame([encoded_sample])\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe294d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_sample[\"text\"].tolist())\n",
    "type(df_sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "592eabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dataset class '''\n",
    "class DicomText(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[[\n",
    "            \"label_modality\",\n",
    "            \"label_vendor\",\n",
    "            \"label_series_type\",\n",
    "            \"label_plane\",\n",
    "            \"label_acquisition\",\n",
    "            \"label_body\",\n",
    "            \"label_contrast\",\n",
    "        ]].to_dict(orient=\"list\")\n",
    "       \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        item = {\"text\": text}\n",
    "        item[\"label_modality\"]   = self.labels[\"label_modality\"][idx]\n",
    "        item[\"label_vendor\"]     = self.labels[\"label_vendor\"][idx]\n",
    "        item[\"label_series_type\"]= self.labels[\"label_series_type\"][idx]\n",
    "        item[\"label_plane\"]      = self.labels[\"label_plane\"][idx]\n",
    "        item[\"label_acquisition\"]= self.labels[\"label_acquisition\"][idx]\n",
    "        item[\"label_body\"]       = self.labels[\"label_body\"][idx]\n",
    "        item[\"label_contrast\"]   = self.labels[\"label_contrast\"][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431eb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'short merge text ...', 'label_modality': 0, 'label_vendor': 1, 'label_series_type': 2, 'label_plane': 0, 'label_acquisition': 1, 'label_body': 3, 'label_contrast': 0}\n",
      "{'text': 'short merge text ...', 'label_modality': 0, 'label_vendor': 1, 'label_series_type': 2, 'label_plane': 0, 'label_acquisition': 1, 'label_body': 3, 'label_contrast': 0}\n"
     ]
    }
   ],
   "source": [
    "data_obj = DicomText(df_sample)\n",
    "for item in data_obj:\n",
    "    print(item)\n",
    "print(data_obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a3694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DicomCollator:\n",
    "    def __init__(self, tokenizer, max_lenght: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_lenght = max_lenght\n",
    "    \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            texts, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_lenght=self.max_lenght,\n",
    "            return_tensor=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_modality     = torch.tensor([item[\"label_modality\"]     for item in batch], dtype=torch.long)\n",
    "        labels_vendor       = torch.tensor([item[\"label_vendor\"]       for item in batch], dtype=torch.long)\n",
    "        labels_series_type  = torch.tensor([item[\"label_series_type\"]  for item in batch], dtype=torch.long)\n",
    "        labels_plane        = torch.tensor([item[\"label_plane\"]        for item in batch], dtype=torch.long)\n",
    "        labels_acquisition  = torch.tensor([item[\"label_acquisition\"]  for item in batch], dtype=torch.long)\n",
    "        labels_body         = torch.tensor([item[\"label_body\"]         for item in batch], dtype=torch.long)\n",
    "        labels_contrast     = torch.tensor([item[\"label_contrast\"]     for item in batch], dtype=torch.long)\n",
    "\n",
    "        enc[\"labels_modality\"]    = labels_modality\n",
    "        enc[\"labels_vendor\"]      = labels_vendor\n",
    "        enc[\"labels_series_type\"] = labels_series_type\n",
    "        enc[\"labels_plane\"]       = labels_plane\n",
    "        enc[\"labels_acquisition\"] = labels_acquisition\n",
    "        enc[\"labels_body\"]        = labels_body\n",
    "        enc[\"labels_contrast\"]    = labels_contrast\n",
    "       \n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713b1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenght = 128\n",
    "collator = DicomCollator(tokenizer, max_lenght)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\n",
    "    data_obj, \n",
    "    batch_size=1, \n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9f73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "attention_mask\n",
      "labels_modality\n",
      "labels_vendor\n",
      "labels_series_type\n",
      "labels_plane\n",
      "labels_acquisition\n",
      "labels_body\n",
      "labels_contrast\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k,v in batch.items():\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c33a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "hidden_size = encoder.config.hidden_size\n",
    "print(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86670e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioBertMultiHead(nn.Module):\n",
    "    def __init__(self, encoder: PreTrainedModel, num_classes_dict: dict):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.modality_head    = nn.Linear(hidden_size, num_classes_dict[\"modality\"])\n",
    "        self.vendor_head      = nn.Linear(hidden_size, num_classes_dict[\"vendor\"])\n",
    "        self.series_type_head = nn.Linear(hidden_size, num_classes_dict[\"series_type\"])\n",
    "        self.plane_head       = nn.Linear(hidden_size, num_classes_dict[\"plane\"])\n",
    "        self.acq_head         = nn.Linear(hidden_size, num_classes_dict[\"acquisition\"])\n",
    "        self.body_head        = nn.Linear(hidden_size, num_classes_dict[\"body\"])\n",
    "        self.contrast_head    = nn.Linear(hidden_size, num_classes_dict[\"contrast\"])\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels_modality=None,\n",
    "        labels_vendor=None,\n",
    "        labels_series_type=None,\n",
    "        labels_plane=None,\n",
    "        labels_acquisition=None,\n",
    "        labels_body=None,\n",
    "        labels_contrast=None,\n",
    "    ):\n",
    "        enc_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = enc_out.pooler_output  # (batch, hidden_size)\n",
    "\n",
    "        logits_modality    = self.modality_head(pooled)\n",
    "        logits_vendor      = self.vendor_head(pooled)\n",
    "        logits_series_type = self.series_type_head(pooled)\n",
    "        logits_plane       = self.plane_head(pooled)\n",
    "        logits_acquisition = self.acq_head(pooled)\n",
    "        logits_body        = self.body_head(pooled)\n",
    "        logits_contrast    = self.contrast_head(pooled)\n",
    "\n",
    "        loss = None\n",
    "        if labels_modality is not None:\n",
    "            loss = 0.0\n",
    "            loss = loss + self.loss_fct(logits_modality, labels_modality)\n",
    "            loss = loss + self.loss_fct(logits_vendor, labels_vendor)\n",
    "            loss = loss + self.loss_fct(logits_series_type, labels_series_type)\n",
    "            loss = loss + self.loss_fct(logits_plane, labels_plane)\n",
    "            loss = loss + self.loss_fct(logits_acquisition, labels_acquisition)\n",
    "            loss = loss + self.loss_fct(logits_body, labels_body)\n",
    "            loss = loss + self.loss_fct(logits_contrast, labels_contrast)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits_modality\": logits_modality,\n",
    "            \"logits_vendor\": logits_vendor,\n",
    "            \"logits_series_type\": logits_series_type,\n",
    "            \"logits_plane\": logits_plane,\n",
    "            \"logits_acquisition\": logits_acquisition,\n",
    "            \"logits_body\": logits_body,\n",
    "            \"logits_contrast\": logits_contrast,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bee6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioBertMultiHead(encoder, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7858aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_lr = 2e-5\n",
    "head_lr    = 5e-5\n",
    "\n",
    "encoder_params = list(model.encoder.parameters())\n",
    "head_params = [\n",
    "    *model.modality_head.parameters(),\n",
    "    *model.vendor_head.parameters(),\n",
    "    *model.series_type_head.parameters(),\n",
    "    *model.plane_head.parameters(),\n",
    "    *model.acq_head.parameters(),\n",
    "    *model.body_head.parameters(),\n",
    "    *model.contrast_head.parameters(),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": encoder_lr},\n",
    "        {\"params\": head_params,    \"lr\": head_lr},\n",
    "    ],\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5250b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52daabee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m, in \u001b[0;36mBioBertMultiHead.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels_modality, labels_vendor, labels_series_type, labels_plane, labels_acquisition, labels_body, labels_contrast)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     19\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     labels_contrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m ):\n\u001b[0;32m---> 30\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m enc_out\u001b[38;5;241m.\u001b[39mpooler_output  \u001b[38;5;66;03m# (batch, hidden_size)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     logits_modality    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_head(pooled)\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/transformers/utils/generic.py:915\u001b[0m, in \u001b[0;36mmerge_with_config_defaults.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m             output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Restore original config value\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/transformers/utils/output_capturing.py:253\u001b[0m, in \u001b[0;36mcapture_outputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Run the forward\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Reset the states\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     _active_collector\u001b[38;5;241m.\u001b[39mreset(output_token)\n",
      "File \u001b[0;32m~/GIT_C/NLP_multihead_multiclass/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:670\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must specify exactly one of input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    671\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        batch = {k: v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49470bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a48e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
